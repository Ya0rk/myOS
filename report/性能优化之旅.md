## fd分配策略优化

在原始方案中，我们只使用一个线性表维护fd，当每次分配新的fd时，为了满足最小化fd原则，需要从线性表的头开始遍历，从实现逻辑来说这确实是最自然的方式，但是`Del0n1x`为了追求更加高效的分配方案，调研了linux中实现，发现linux使用了位图机制，这样可以快速定位到未分配的fd。有了linux的设计启发，`Del0n1x`采用位图+缓存的方式对目前的fd分配策略做出了优化。

`FdTable`设计如下：
```
#[derive(Clone)]
pub struct FdTable {
    pub table: Vec<FdInfo>, // 将fd作为下标idx
    pub rlimit: RLimit64,
    free_bitmap: Vec<u64>,   // 空闲FD位图 (1表示空闲, 0表示已使用)
    next_free: usize,        // 快速查找起点
    freed_stack: Vec<usize>, // 保存最近释放的FD缓存
}
```

使用free_bitmap位图记录当前table表中空闲和占用清空；next_free记录最小的空闲fd其实位置，这样不用每次都从0开始遍历；freed_stack中保存最近释放的fd；

在`alloc_fd`中，首先从freed_stack中找，如果未找到就通过位图快速查找空闲位置，如果都没有找到，那么只能扩展table表，将最后一个下标作为new_fd;当然，每次找到后需要更新位图，同时在删除fd时也要更新。

优化后lmbench性能有所提升：
`Del0n1x`:
```
Simple syscall: 37.0562 microseconds
Simple read: 47.7990 microseconds
Simple write: 49.7663 microseconds
```


## `lmbench`中stat和open/close优化

`./lmbench_all lat_syscall -P 1 stat /var/tmp/lmbench`,测试stat系统调用延迟;`./lmbench_all lat_syscall -P 1 open /var/tmp/lmbench`,测试open系统调用 open 延迟。化前，测试结果为：`Simple stat: 3240. 0266 microseconds
` `Simple open/close: 2381 .9258 microseconds
`；后来我们发现性能瓶颈存在于dentry中还没有实现`inode cache`，导致每次寻找文件都需要从目录树的root开始，有极大的提升空间；后来利用`inode cache`，当是第一次访问文件时，需要将其加入`inode cache`中；在`get_dentry_from_path`中，每次调用前首先访问cache，如果存在目标文件的dentry，就直接返回。

优化后的效果：`Simple stat: 1626.7143 microseconds`性能提高50%；Simple open/close: 322.6966 microseconds`，性能提高7倍！！